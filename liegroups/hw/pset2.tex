\documentclass{../../mathnotes}

\usepackage{tikz-cd}
\usepackage{todonotes}

\title{Lie Groups PSET 2}
\author{Nilay Kumar}
\date{Last updated: \today}


\begin{document}

\maketitle

\begin{prop}
    Kirillov 2.15
\end{prop}
\begin{proof}
    Let us first show that $\End_\HH\HH^n$ is naturally identified with the algebra of $n\times n$ quaternionic matrices.
    Let $\left\{ e_\alpha \right\}$ be an orthonormal basis for $\HH^n$ over $\HH$ (the orthonormality condition will be useful later).
    Then we may write any $\vec h=\sum_\alpha e_\alpha h_\alpha$, as we are considering
    $\HH^n$ as a right $\HH$-module. Applying an endomorphism $A$, we find that $A(\vec h)=A\left( \sum_\alpha e_\alpha h_\alpha \right)
    =\sum_\alpha A(e_\alpha h_\alpha)=\sum_\alpha A(e_\alpha) h_\alpha$. Denoting the $\beta$ component of $A(e_\alpha)$ by the quaternion $A_{\beta\alpha}$,
    we can rewrite
    \[A(\vec h)=\sum_{\alpha,\beta}e_\beta A_{\beta\alpha}h_\alpha.\]
    From this, it is clear that the endomorphism is characterized precisely by this $n\times n$ matrix $A_{\beta\alpha}$ of quaternions.

    Now let us define an $\HH$-valued form on $\HH^n$ given by
    \[(\vec h,\vec h')=\sum_i\bar h_i h_i'\]
    and let $U(n,\HH)$ be the group of unitary quaternionic transformations:
    \[U(n,\HH)=\left\{ A\in\End_\HH\HH^n | (A\vec h,A\vec h')=(\vec h,\vec h') \right\}.\]
    It's clear that $U(n,\HH)$ is indeed a group - composing two transformations yields another such transformation, and the identity is contained in the
    group. The inverses are given by just the matrix inverses (that clearly exist because no $A\in U(n,\HH)$ has determinant 0):
    \[(\vec h,\vec h')=(AA^{-1}\vec h,AA^{-1}\vec h')=(A^{-1}\vec h,A^{-1}\vec h').\]
    Using the identification of endomorphisms with matrices that we established in the previous paragraph, let us find a matrix characterization
    for elements of $U(n,\HH)$:
    \begin{align*}
        (A\vec h, A\vec h')&=\left( \sum_{\alpha,\beta}e_\beta A_{\beta\alpha}h_{\alpha},\sum_{\gamma,\delta}e_\gamma A_{\gamma\delta} h'_\delta \right)\\
        &=\sum_{\beta,\gamma}(e_\beta,e_\gamma)\left( \overline{\sum_\alpha A_{\beta\alpha}h_\alpha} \right)\left( \sum_\delta A_{\gamma\delta}h_\delta' \right)\\
        &=\sum_{\alpha,\beta,\delta}\overline{A_{\beta\alpha} h_\alpha}A_{\beta\delta}h'_\delta
        =\sum_{\alpha,\beta,\delta}\overline{h_\alpha}\cdot\overline{A_{\beta\alpha}}A_{\beta\delta}h'_\delta.
    \end{align*}
    But this is precisely $(\vec h,\vec h')$ if and only if the middle terms sum to the identity, i.e. $A^*A=1$.

    Define now a map $\phi:\C^{2n}\to\HH^n$ given by $(z_1,\ldots, z_{2n})\mapsto(z_1+jz_{n+1},\ldots,z_n+jz_{2n})$. If we treat $\HH^n$ as a complex vector
    space via the scalar multiplication $z(h_1,\ldots,h_n)=(h_1z,\ldots,h_nz)$, $\phi$ is in fact an isomorphism of complex vector spaces. To see this, it suffices
    to show that $\phi$ is injective, but this is obvious because $z_i+jz_{n+1}=a_i+ib_i+ja_{n+i}-kb_{n+i}=0$ implies that the $a_i=a_{n+i}=b_i=b_{n+i}=0$,
    and hence that $z_i=z_{n+i}=0$, i.e. $\ker\phi=\{\vec 0\}$. Next let us show that this isomorphism identifies $\End_\HH\HH^n$ with
    $\{A\in\End_\C\C^{2n} | \bar A=J^{-1}AJ\}$ where
    \[J=\begin{pmatrix}0&-I_n\\I_n&0\end{pmatrix}.\]
    First note that the map $\vec h\mapsto\vec hj$ is identified with $\vec z\mapsto J\vec{\bar z}$, because quaternionic multiplcation by $j$ is equivalent
    to multiplying $(z_i+jz_{n+i})j=z_ij+jz_{n+i}j=-\bar z_{n+i}+j\bar z_i$.

    Under the identification $\C^{2n}\cong\HH^n$ above, the quaternionic form simplifies as follows. Let $h_i=z_i+jz_{n+i}$ and $h'_i=z_i'+jz_{n+1}'$.
    Then, by definition,
    \begin{align*}
        (\vec h,\vec h')&=\sum_l (\bar z_l+\overline{jz_{n+l}})(z_l'+jz_{n+l}')\\
        &=\sum_l\left( \bar z_lz_l'+\bar z_{n+l}z'_{n+l}-\bar z_{n+l}jz'_l+\bar z_l j z'_{n+l} \right).
    \end{align*}
    Anti-commuting the $j$ and factoring it out of the last two terms, we see that the first term is the usual Hermitian inner product and the last terms
    are the standard bilinear skew-symmetric form in $\C^{2n}$ (multiplied by $j$). For the whole form to be preserved, each form must be preserved, and hence
    we see that $Sp(n)=Sp(n,\C)\cap SU(2n)$.
\end{proof}

\begin{prop}
    Kirillov 3.16
\end{prop}
\begin{proof}
    We wish to show that $\fr{sp}(n)_\C=\fr{sp}(n,\C)$. Note that (from Kirillov) $\fr{sp}(n,\C)$ is the set of $2n\times 2n$ complex matrices $x$ such that $x+J^{-1}x^tJ=0$.
    As $Sp(n)$ is the intersection of $Sp(n,\C)$ and $SU(2n)$, it's clear that $\fr{sp}(n)$ is the set of matrices that satsify this condition in addition
    to the condition that $x^\dagger=-x$. If we now complexify and consider $\fr{sp}(n)_\C=\fr{sp}(n)\oplus i\fr{sp}(n)$, the first condition $x+J^{-1}x^tJ=0$
    still holds (by linearity) but the second no longer holds, as $(ix)^\dagger=-ix^\dagger=ix$ instead of $-ix$. This is precisely what we wanted to show.
\end{proof}

\begin{prop}
    Kirillov 3.1
\end{prop}
\begin{proof}
    Let $X=\begin{pmatrix}-1&1\\0&-1\end{pmatrix}$. Suppose $x\in\fr{sl}(2,\R)$ such that $\exp(x)=X$. It's clear that the eigenvalues of $x$
    must exponentiate to the eigenvalues of $X$, as eigenvalues of powers are simply powers of eigenvalues. Hence we see that if $\lambda_1,\lambda_2\in\C$
    are the eigenvalues of $x$ then $e^{\lambda_1}=e^{\lambda_2}=-1$. This yields
    \begin{align*}
        \lambda_1&=(2n+1)\pi i\\
        \lambda_2&=(2m+1)\pi i
    \end{align*}
    for some $m,n\in\Z$. The condition that $x\in\fr{sl}(2,\R)$ requires that $\lambda_1+\lambda_2=0$, i.e. that $m=-(n+1)$. Hence the $\lambda_1,\lambda_2$ must be
    distinct, i.e. diagonalizable over the complex numbers. In other words, we have that $P^{-1}xP=\lambda$ where $\lambda$ is diagonal with $\lambda_1,\lambda_2$.
    Exponentiating, we find that
    \begin{align*}
        P^{-1}\exp(x)P=\begin{pmatrix}-1&0\\0&-1\end{pmatrix}.
    \end{align*}
    Moving the $P$'s to the other side, we find that $\exp(x)=-I$, which is clearly not equal to $X$. Hence we obtain a contradiction - there can exist no such $x$.
\end{proof}

\begin{prop}
    Kirillov 3.5
\end{prop}
\begin{proof}
    Consider $\R^3$ as a vector space with a Lie bracket given by the cross-product. It's clear that this is indeed a Lie bracket, because
    for $\vec v,\vec w,\vec u\in\R^3$, the cross product is clearly bilinear, and satisfies $\vec v\times\vec w=-\vec v\times \vec w$. The Jacobi identity
    follows by first noting that for $\vec a,\vec b,\vec c\in\R^3$, the triple vector product rule gives us $\vec a\times (\vec b\times \vec c)=\vec b(\vec a\cdot \vec c)-c(\vec a\cdot \vec b)$ where
    the product is the usual inner product. Then, permuting the orders, we find that
    \begin{align*}
       \vec a\times(\vec b\times \vec c)+\vec b\times(\vec c\times\vec a)+\vec c\times(\vec a\times\vec b)=0 
    \end{align*}
    where the cancellations are acheieved by the commutativity of the inner product. Hence $\R^3$ with the cross product forms a Lie algebra.

    Now consider the usual basis for $\fr{so}(3,\R)$ denoted by $J_x,J_y,J_z$ and the basis for $\R^3$ denoted by $x,y,z$. Let $\phi$ be the vector
    isomorphism that takes $J_x$ to $x$, $J_y$ to $y$, and $J_z$ to $z$. Moreover, $\phi$ is a Lie algebra homomorphism as one can check (it is straightforward
    but tedious), as the basis of $\fr{so}(3,\R)$ follows precisely the same commutation relations as does the basis for $\R^3$.

    Consider the standard action of $\fr{so}(3,\R)$ on $\R^3$ given by matrix multiplication $a\cdot\vec v=\phi(a)\times\vec v$. We wish to show that
    this action coincides with the action of $\R^3$ on itself via the cross product: $a\cdot\vec v=\phi(a)\times \vec v$. To do this, first note that:
    \begin{align*}
        J_x\cdot x&=\begin{pmatrix}0&0&0\\0&0&-1\\0&1&0\end{pmatrix}
            \begin{pmatrix}
                1\\0\\0
            \end{pmatrix}
            =\vec 0\\
        J_x\cdot y&=z\\
        J_x\cdot z&=-y\\
        J_y\cdot x&=\begin{pmatrix}0&0&1\\0&0&0\\-1&0&0\end{pmatrix}
            \begin{pmatrix}
                1\\0\\0
            \end{pmatrix}
            =-z\\
        J_y\cdot y&=\vec 0\\
        J_y\cdot z&=x\\
        J_z\cdot x&=\begin{pmatrix}0&-1&0\\1&0&0\\0&0&0\end{pmatrix}
            \begin{pmatrix}
                1\\0\\0
            \end{pmatrix}
            =y\\
        J_z\cdot y&=-x\\
        J_z\cdot z&=\vec 0
    \end{align*}
    which we can use to simplify
    \begin{align*}
        a\cdot v&=\left( \alpha J_x+\beta J_y+\gamma J_z \right)\cdot (v_xx+v_yy+v_zz)\\
        &=\alpha\left( v_yz-v_zy \right)+\beta\left( -v_xz+v_zx \right)+\gamma\left( v_xy-v_yx \right)\\
        &=\left( \beta v_z-\gamma v_y \right)x+\left( \gamma v_x-\alpha v_z \right)y+\left( \alpha v_y-\beta v_z \right)z.
    \end{align*}
    But this is precisely the formula for the cross product of the two vectors $\phi(a)$ and $v$, as desired.
\end{proof}


\begin{prop}
    Kirillov 3.8
\end{prop}
\begin{proof}
    Consider the open cell $U=\C\subset\CP^1$ with coordinates given by $t=x/y$. The action of $SL(2,\C)$ on a point in $U$ is given by
    \begin{align*}
        A(x:y)=\begin{pmatrix}
            a & b\\c & d
        \end{pmatrix}
        (x:y)=(ax+by:cx+dy)=(at+b:ct+d)
    \end{align*}
    which we will denote by $[\frac{at+b}{ct+d}]$. Denoting the representation as $\pi$, we can consider the usual action of $SL(2,\C)$ on functions
    defined on $U$:
    \begin{align*}
        \pi(A)(f)([t])=f(\pi(A^{-1})[t]).
    \end{align*}
    The induced action of $\fr g$ by vector fields on functions defined on $\CP^1$ is then given:
    \begin{align*}
        \pi'(X)(f)([t])=\frac{d}{ds}\bigg|_{s=0}f(\pi(e^{-sX})[t])=\frac{d}{ds}\bigg|_{t=0}\left( \pi(e^{-sX})([t]) \right)\frac{df}{dt}([t]),
    \end{align*}
    which we can evaluate for the basis $\left\{ H,X,Y \right\}\subset \fr{sl}_2\C$:
    \begin{align*}
        H=\begin{pmatrix}1&0\\0&-1\end{pmatrix},\;
            X=\begin{pmatrix}0&1\\0&0\end{pmatrix},\;
                Y=\begin{pmatrix}0&0\\1&0\end{pmatrix}
    \end{align*}
    whose exponentials yield:
    \begin{align*}
        \exp(-sH)=\begin{pmatrix}e^{-s}&0\\0&e^{s}\end{pmatrix},\;
            \exp(-sX)=\begin{pmatrix}1&-s\\0&1\end{pmatrix},\;
                \exp(-sY)=\begin{pmatrix}1&0\\-s&1\end{pmatrix}.
    \end{align*}
    Let us now compute the coefficient in front of $df/dt$ in the expression for $\pi'$ above for the various basis elements:
    \begin{align*}
        \frac{d}{ds}\bigg|_{t=0}\pi\begin{pmatrix}e^{-s}&0\\0&e^{s}\end{pmatrix}([t])&=\frac{d}{ds}\bigg|_{t=0}\left[ e^{-2s}t \right]=[-2t]\\
        \frac{d}{ds}\bigg|_{t=0}\pi\begin{pmatrix}1&-s\\0&1\end{pmatrix}([t])&=\frac{d}{ds}\bigg|_{t=0}\left[ t-s \right]=[-1]\\
            \frac{d}{ds}\bigg|_{t=0}\pi\begin{pmatrix}1&0\\-s&1\end{pmatrix}([t])&=\frac{d}{ds}\bigg|_{t=0}\left[ \frac{t}{-st+1} \right]=[t^2].
    \end{align*}
    Hence we see, in local coordinates $t$ for $U$, that the vector fields corresponding to $H,X,Y$ are precisely
    \begin{align*}
        H&\longleftrightarrow -2t\frac{d}{d t}\\
        X&\longleftrightarrow -\frac{d}{d t}\\
        Y&\longleftrightarrow t^2\frac{d}{d t}.
    \end{align*}
\end{proof}




\end{document}
